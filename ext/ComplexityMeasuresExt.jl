module ComplexityMeasuresExt
using Statistics
using LinearAlgebra
using ComplexityMeasures
using TimeseriesBase # For windowing
using FractionalNeuralSampling
import FractionalNeuralSampling: samplingpower, samplingaccuracy, _samplingaccuracy

"""
The variance of differences scaled by the time step.
Also known as the quadratic variation divided by the number of samples
For p != 2, this uses other norms of the increments (generalized variation).
"""
function samplingpower(x, dt; p=2)
    # # # Compute squared increments
    # # dxÂ² = map(Base.Fix1(sum, abs2), diff(x))

    # # # Return energy input rate
    # # return mean(dxÂ²) / dt
    # return var(diff(x)) / dt

    Î”x = diff(x)
    p_var = sum(abs.(Î”x) .^ p)
    T = length(x) * dt
    return (p_var / T)^(1 / p)
end

function _samplingaccuracy(x, ğœ‹::AbstractDensity, Ï„s::AbstractVector=2:100; p=10)
    if minimum(Ï„s) < 2
        error("Minimum Ï„ (samples) must be at least 2")
    end

    P = map(logdensity(ğœ‹), x)

    S = Base.Fix1(information, Kraskov(Shannon(; base=â„¯); k=3))

    map(Ï„s) do Ï„

        # * KL divergence can be estimated from cross entropy and sample entropy of
        # * trajectory segments

        # * Calculate the cross-entropy part, which is independent of any windowing
        CE = -map(mean, window(P, Ï„, p))

        # ** Estimate the differential entropy of each window
        DE = map(S, window(x, Ï„, p))

        # * Approximate the final KL divergence
        Î”I = CE - DE
    end
end

function samplingaccuracy(x, ğœ‹::AbstractDensity, Ï„s::AbstractVector=2:100; kwargs...)
    Î”I = _samplingaccuracy(x, ğœ‹, Ï„s; kwargs...)
    return map(x -> exp.(-x), Î”I)
end

end
