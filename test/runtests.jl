using Test
using TestItems
using TestItemRunner

@run_package_tests

@testitem "Aqua.jl" begin
    using Aqua
    Aqua.test_all(FractionalNeuralSampling)
end

@testsnippet Setup begin
    using CairoMakie
    using Foresight
    using DifferentialEquations
    using FractionalNeuralSampling
    import FractionalNeuralSampling as FNS
    using Random
    using Statistics
    using StaticArraysCore
    using Test
    using Distributions
    using DiffEqNoiseProcess
    using StableDistributions
    using BenchmarkTools
    using Profile
    using LinearAlgebra
    using ForwardDiff
    using LogDensityProblems
    using DifferentiationInterface
    using BenchmarkTools
    using InteractiveUtils
    using Distances
    using Distributed
    using SpecialFunctions
    using FileIO
    using StatsBase
    using Autocorrelations
    using TimeseriesTools
    import TimeseriesTools.msdist
    using Normalization
    import FractionalNeuralSampling: Density
    set_theme!(foresight(:physics))
end

@testitem "Density" setup=[Setup] begin
    # Use a normal pdf
    f(x) = 1 / sqrt(2œÄ) * exp(-x^2 / 2)
    f(x::AbstractVector{T}) where {T} = f(only(x))::T # Ensure univariate consistency
    D = @inferred Density{typeof(f), 1, false}(f) # 1D, no autodiff
    D = @inferred Density{1, false}(f) # 1D, no autodiff
    D = Density{1}(f, false) # Not type stable??
    @test_throws "MethodError" gradlogdensity(D, 0.0) # No autodiff, so errors

    D = @inferred Density{1}(f) # 1D, do autodiff
    d = @inferred D(0.0)
    logd = @inferred logdensity(D, 0.0)
    glogd = @inferred gradlogdensity(D, 0.0)
    @inferred gradlogdensity(D, [0])
    @test_throws "ArgumentError" gradlogdensity(D, [0.0, 0.0])
    @inferred gradlogdensity(D, [[0.0], [1.0]])

    @inferred potential(D, 0.0)
    @inferred gradpotential(D, 0.0)
    @test gradpotential(D) isa Function

    @inferred FractionalNeuralSampling.Densities.logdensity_and_gradient(D, 0.0)
end

@testitem "Langevin sampler bias" setup=[Setup] begin
    u0 = [0.0001 0.0001]
    tspan = (0.0, 1000.0)
    dt = 0.01
    D = FNS.Density(Normal(0, 1))
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1.0, Œ≥ = 1.0, ùúã = D,
                            noise = WienerProcess(0.0, 0.0))

    W = @test_nowarn remake(S, p = S.p)
    @test_nowarn solve(W, EM(); dt, saveat = 0.01)
    @test W.p == S.p
    @test W.u0 == S.u0
    @test W.tspan == S.tspan
    @test W.f == S.f
    @test W.g == S.g

    @test_nowarn KLDivergence()(D, randn(100))
    sol = solve(S, EM(); dt)
    Makie.density(first.(sol.u))
    lines!(-2.5:0.1:2.5, D.(-2.5:0.1:2.5))
    current_figure()
    @test mean(first.(sol.u))‚âà0.0 atol=0.05
    @test std(first.(sol.u))‚âà1.0 atol=0.05

    if false
        tspan = (0.0, 100.0)
        S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1.0, Œ≥ = 1.0, ùúã = D,
                                noise = WienerProcess(0.0, 0.0))
        Œ≤s = range(0, 5, length = 10)
        er = map(Œ≤s) do Œ≤
            P = remake(S, p = ((Œ≤, S.p[1][2:end]...), S.p[2:end]...))
            ensemble = EnsembleProblem(P)
            sol = solve(ensemble, EM(); dt, trajectories = 1000)
            ts = [s[1, :] for s in sol]
            er = evaluate.([KLDivergence()], [D], ts) |> mean
            s = std.(ts) |> mean
            return (er, s)
        end
        stds = last.(er)
        ers = first.(er)
        lines(Œ≤s, ers)
        lines(Œ≤s, stds)
    end
end

@testitem "Levy sampler bias" setup=[Setup] begin
    u0 = [0.0 0.0]
    tspan = (0.0, 500.0)
    dt = 0.01
    D = FNS.Density(MixtureModel(Normal, [(-2, 0.5), (2, 0.5)]))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.2, Œ≤ = 0.1, Œ≥ = 0.5, ùúã = D)

    W = @test_nowarn remake(S, p = S.p)
    @test_nowarn solve(W, EM(); dt, saveat = 0.01)
    @test W.p == S.p
    @test W.u0 == S.u0
    @test W.tspan == S.tspan
    @test W.f == S.f
    @test W.g == S.g

    sol = solve(S, EM(); dt)
    x = first.(sol.u)
    x = x[abs.(x) .< 6]
    density(x)
    lines!(-4:0.1:4, D.(-4:0.1:4))
    current_figure()
    @test evaluate(KLDivergence(), D, x) < 0.1

    if false
        Œ≤s = range(0, 2, length = 20)
        er = map(Œ≤s) do Œ≤
            P = remake(S, p = ((S.p[1][1], Œ≤, S.p[1][3]), S.p[2:end]...))
            ensemble = EnsembleProblem(P)
            sol = solve(ensemble, EM(); dt, trajectories = 10)
            ts = map(sol) do s
                x = s[1, :]
                x = x[abs.(x) .< 6]
            end
            er = evaluate.([KLDivergence()], [D], ts) |> mean
            return er
        end
        lines(Œ≤s, er)

        Œ±s = range(1.1, 2.0, length = 20)
        er = map(Œ±s) do Œ±
            P = remake(S, p = ((Œ±, S.p[1][2:end]...), S.p[2:end]...))
            ensemble = EnsembleProblem(P)
            sol = solve(ensemble, EM(), EnsembleThreads(); dt, trajectories = 100)
            ts = map(sol) do s
                x = s[1, :]
                x = x[abs.(x) .< 6]
            end
            er = evaluate.([KLDivergence()], [D], ts) |> mean
            return er
        end
        lines(Œ±s, er)

        Œ±s = range(1.25, 2.0, length = 52)
        Œ≤s = range(0, 2, length = 51)
        ps = Iterators.product(Œ±s, Œ≤s) .|> collect
        push!.(ps, 1.0) # Add Œ≥
        ers = pmap(ps) do p
            P = remake(S, p = (Tuple(p), S.p[2:end]...))
            ensemble = EnsembleProblem(P)
            sol = solve(ensemble, EM(); dt, trajectories = 1000)
            ts = [s[1, :] for s in sol]
            ts = map(sol) do s
                x = s[1, :]
                x = x[abs.(x) .< 6]
            end
            er = evaluate.([KLDivergence()], [D], ts) |> mean
            return er
        end
        fax = heatmap(Œ±s, Œ≤s, (ers); axis = (; xlabel = "Œ±", ylabel = "Œ≤"))
        Colorbar(fax.figure[1, 2], fax.plot; label = "KL Divergence")
        fax
        # heatmap(Œ±s, Œ≤s, stds; axis = (; xlabel = "Œ±", ylabel = "Œ≤"))
    end
end

@testitem "Recursive Arrays" setup=[Setup] begin
    # So for recursive arrays, we can use diagonal noise by setting the noise_rate prototype
    # to similar(u0)
    u0 = ArrayPartition([0.0], [0.0])
    tspan = (0.0, 500.0)
    dt = 0.01
    D = FNS.Density(MixtureModel(Normal, [(-2, 0.5), (2, 0.5)]))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.2, Œ≤ = 0.1, Œ≥ = 0.5, ùúã = D)

    W = @test_nowarn remake(S, p = S.p)
    @test_nowarn solve(W, EM(); dt, saveat = 0.01)
    @test W.p == S.p
    @test W.u0 == S.u0
    @test W.tspan == S.tspan
    @test W.f == S.f
    @test W.g == S.g

    sol = solve(S, EM(); dt)
    x = first.(sol.u)
    x = x[abs.(x) .< 6]
    @test evaluate(KLDivergence(), D, x) < 0.1
end

if false
    u0 = [-3.0 0.00]
    tspan = (0.0, 5000.0)
    dt = 0.01
    D = FNS.Density(MixtureModel(Laplace, [(-3, 0.3), (3, 0.3)]))
    # D = FNS.Density(Normal(-2, 0.5))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.5, Œ≤ = 0.0, Œ≥ = 0.1, ùúã = D)

    sol = solve(S, EM(); dt)
    x = first.(sol.u)
    x = x[abs.(x) .< 8]
    density(filter(!isnan, x))
    lines!(-4:0.1:4, D.(-4:0.1:4))
    current_figure()

    g = Figure(size = (800, 400))
    ax = Axis(g[1, 1])
    lines!((1:10000), x[1:10000])
    ax = Axis(g[1, 2])
    xx = Timeseries((1:length(x)), x)
    lines!((1:5000), autocor(centraldiff(xx), 1:5000))
    # spectrumplot(spectrum(xx, 1))
    g
end

if false # * Simple potential: power law iqr?
    u0 = [-0.001 0.00]
    tspan = (0.0, 1.0)
    dt = 0.00001
    D = FNS.Density(Normal(0, 1))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.2, Œ≤ = 0.0, Œ≥ = 0.1, ùúã = D)
    P = EnsembleProblem(S)
    sol = solve(P, EM(); dt, trajectories = 100)
    œÉ = mapslices(iqr, stack(getindex.(sol.u, 1, :)); dims = 2)[:] .^ 2

    mm = first([log10.(2:1000) ones(length(œÉ[2:1000]))] \ log10.(œÉ[2:1000]))

    @test 1.6 < mm < 1.7

    lines(œÉ[2:10000]; axis = (; xscale = log10, yscale = log10))
    lines!((2:10000) ./ 1e8)
    current_figure()
end

if false # * Unimodal vs bimodal comparison
    Random.seed!(43)
    g = Figure(size = (720, 360))
    u0 = [-0.001 0.00]
    tspan = (0.0, 1000.0)
    dt = 0.001
    idxs = range(start = 1, step = 200, length = 500)
    # ft = identity
    ft = x -> abs.((x[3:end] .- x[1:(end - 2)]) ./ 2)

    ax = Axis(g[1, 1]; xlabel = "t", ylabel = "v", title = "Unimodal")
    D = FNS.Density(Laplace(0, 0.5))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.4, Œ≤ = 2.0, Œ≥ = 0.5, ùúã = D)
    sol = solve(S, EM(); dt)
    x = sol[1, :][idxs]
    lines!(ax, ft(x), color = :cornflowerblue, linewidth = 2)

    ax = Axis(g[1, 2])
    hist!(ax, ft(x); direction = :x, bins = 50, color = :gray)
    hidedecorations!(ax)
    hidespines!(ax)
    tightlimits!(ax)

    u0 = [-0.201 0.00]
    ax = Axis(g[2, 1]; xlabel = "t", ylabel = "v", title = "Bimodal")
    D = FNS.Density(MixtureModel([Laplace(-1, 0.5), Laplace(1, 0.5)]))
    S = FNS.FractionalNeuralSampler(; u0, tspan, Œ± = 1.4, Œ≤ = 2.0, Œ≥ = 0.5, ùúã = D)
    sol = solve(S, EM(); dt)
    x = sol[1, :][idxs]

    lines!(ax, ft(x); color = :crimson, linewidth = 2)

    ax = g[2, 2] |> Axis
    hist!(ax, ft(x); direction = :x, bins = 50, color = :gray)
    hidedecorations!(ax)
    hidespines!(ax)
    tightlimits!(ax)

    colgap!(g.layout, 1, Relative(0))
    colsize!(g.layout, 2, Relative(0.2))
    linkyaxes!(contents(g.layout)...)
    g
end

if false # * Fixation simulation: heavy tailed msd??
    u0 = [-0.001 0.00]
    tspan = (0.0, 100.0)
    dt = 0.001
    D = FNS.Density(Laplace(0, 1))
    S = FNS.FractionalHMC(; u0, tspan, Œ± = 2.0, Œ≤ = 0.1, Œ≥ = 0.1, ùúã = D)

    sol = solve(S, EM(); dt)
    x = first.(sol.u)[1:50:end] # Need heavy oversampling to prevent blowout
    x = x[abs.(x) .< 8]
    density(filter(!isnan, x))
    lines!(-4:0.1:4, D.(-4:0.1:4))
    # current_axis().yscale = log10
    current_axis().limits = (nothing, (1e-3, nothing))
    current_figure()

    d = map(1:5000) do t
        mean((x[1:(end - t)] .- x[(t + 1):end]) .^ 2)
    end
    plot(d; axis = (; xscale = log10, yscale = log10))
    a = d[1:100]
    b = hcat(ones(length(a)), log.(1:length(a))) \ log.(a)
    lines!(1:100, exp(b[1]) * (1:100) .^ b[2]; color = :red)
    text!(1, 1; text = "Œ±=$(b[2])")
    current_figure()

    lines(x[1:10:1000])

    c = collect(centraldiff(centraldiff(centraldiff(centraldiff(centraldiff(Timeseries(1:length(x),
                                                                                       x)))))))
    c = c[abs.(c) .< 0.01]
    hist(c, bins = 100, normalization = :pdf)
    g = fit(Stable, c)
    lines!(-0.005:0.00001:0.005, pdf.([g], -0.005:0.00001:0.005))
    current_figure()
end

@testitem "Autodiff" setup=[Setup] begin
    D = Normal(0, 0.5)
    @inferred logpdf(D, 0.1)
    g(x::T) where {T <: Real} = logpdf(D, x)::T
    g(x::AbstractVector{T}) where {T <: Real} = logpdf(D, only(x))::T
    @inferred g(0.1)
    @inferred ForwardDiff.gradient(g, [0.1])
    backend = AutoForwardDiff()
    @test gradlogdensity(FNS.Density(D, true)).(0.1:0.1:3) ==
          gradlogpdf.([D], 0.1:0.1:3)
    gr = @inferred gradient(g, backend, [0.1])
    @test gr isa Vector
    @test length(gr) == 1
    a = @benchmark gradient($g, $backend, [0.1])
    b = @benchmark ForwardDiff.gradient($g, [0.1])
    c = @benchmark gradlogpdf($D, 0.1)
    @test a.allocs < 15
    @test b.allocs < 10
    @test c.allocs == c.memory == 0
    @inferred gradlogdensity(FNS.Density(D, false), 0.1)
    a = @benchmark gradlogdensity(FNS.Density($D, false), 0.1)
    @test a.allocs == c.memory == 0
    @inferred gradlogdensity(FNS.Density(D, true), 0.1)
    b = @benchmark gradlogdensity(FNS.Density($D, true), 0.1)
    @test b.allocs == 10 # Slightly allocating

    cl = @code_lowered FNS.Densities._gradlogdensity(FNS.Density(D, true), 0.1)
    @test contains(string(cl.code), "AD_BACKEND")
end
@testitem "Univariate DistributionDensity" setup=[Setup] begin
    d = Normal(0.0, 0.5)
    D = @test_nowarn FNS.Density(d)
    @test D isa FNS.Densities.AbstractUnivariateDensity
    @test FNS.Densities.doautodiff(D) == false
    @test D(0.0) == 2 / sqrt(2œÄ)
    @test D([0.0]) == 2 / sqrt(2œÄ)
    @test LogDensityProblems.dimension(D) == 1
    @test all(map(LogDensityProblems.logdensity(D), -1:0.1:1) .‚âà log.(D.(-1:0.1:1)) .‚âà
              logpdf(distribution(D), -1:0.1:1))
    @inferred LogDensityProblems.logdensity(D, 0.0)
    @inferred LogDensityProblems.logdensity(D, 0)
    lines(-2:0.1:2, D.(-2:0.1:2))
    lines(-2:0.01:2, FNS.Densities.potential(D).(-2:0.01:2))
    @inferred FNS.Densities.gradlogdensity(D, 0.01)
    @inferred map(FNS.Densities.gradlogdensity(D), 0.01:0.01:5)
    @test map(FNS.Densities.gradlogdensity(D), 0.1:0.1:5) == gradlogpdf.([d], 0.1:0.1:5)

    d = Uniform(-0.5, 0.5)
    D = @test_nowarn FNS.Density(d)
    @test D(0.0) == 1
    @test LogDensityProblems.dimension(D) == 1
    @test all(map(LogDensityProblems.logdensity(D), -1:0.1:1) .‚âà log.(D.(-1:0.1:1)))
    @inferred LogDensityProblems.logdensity(D, 0.0)
    @inferred LogDensityProblems.logdensity(D, -0.6)
    @inferred LogDensityProblems.logdensity_and_gradient(D, -0.6)
    @inferred LogDensityProblems.logdensity_and_gradient(FNS.Density(d, true), 0.5)

    if isinteractive()
        @benchmark map(FNS.Densities.gradlogdensity($D), -1:0.01:1)
        @benchmark LogDensityProblems.logdensity_and_gradient.([$D], -1:0.01:1)
    end
    lines(-2:0.1:2, D.(-2:0.1:2))
    lines(-2:0.01:2, FNS.Densities.potential(D).(-2:0.01:2))
    lines(-2:0.01:2, FNS.Densities.gradlogdensity(D).(-2:0.01:2))

    D = @test_nowarn FNS.Density(Normal(0.0f0, 0.5f0))
    @test LogDensityProblems.logdensity(D, 0.0f0) isa Float32
    @test FNS.Densities.gradlogdensity(D, 0.0f0) isa Float32
end

@testitem "Multivariate DistributionDensity" setup=[Setup] begin
    N = 3
    Œºs = randn(N)
    x = randn(N, 100)
    Œ£ = x * x'
    d = MvNormal(Œºs, Œ£)
    D = @test_nowarn FNS.Density(d)
    @test D isa FNS.Densities.DistributionDensity
    @test !(D isa FNS.Densities.AbstractUnivariateDensity)
    @test FNS.Densities.doautodiff(D) == false
    p = randn(N)
    @test logdensity(D)(p) == logpdf(d, p)
    ps = eachcol(randn(N, 100))
    @test logdensity(D)(ps) == logpdf.([d], ps)
    @test gradlogdensity(D)(p) == gradlogpdf(d, p)
    @test map(gradlogdensity(D), ps) == gradlogpdf.([d], ps)

    # * Ad
    D = @test_nowarn FNS.Density(MvNormal(Œºs, Œ£), true)
    @test FNS.Densities.doautodiff(D) == true
    @test LogDensityProblems.logdensity(D, p) isa Float64
    @test logdensity(D)(p) == logpdf(d, p)
    @test logdensity(D)(ps) == logpdf.([d], ps)
    @test gradlogdensity(D)(p) ‚âà gradlogpdf(d, p)
end

@testitem "Mixture DistributionDensity" setup=[Setup] begin
    Nd = 3
    N = 10
    Œºs = [randn(Nd) for _ in 1:N]
    Œ£s = map(1:N) do i
        x = randn(Nd, 100)
        x * x'
    end
    d = MixtureModel([MvNormal(Œºs[i], Œ£s[i]) for i in 1:N])
    D = @test_nowarn FNS.Density(d)
    @test D isa FNS.Densities.AdDensity
    p = rand(D) # Draw from the distribution
    @test logdensity(D)(p) == logpdf(d, p)
    ps = eachcol(randn(Nd, 100))
    @test logdensity(D)(ps) == logpdf.([d], ps)
    @test gradlogdensity(D)(p) isa Vector{Float64}

    d = MixtureModel([Normal(0, 1), Normal(0, 0.5)])
    D = @test_nowarn FNS.Density(d)
    @test FNS.Densities.doautodiff(D) == true
    @test gradlogdensity(D)(0.0) == 0.0
end

@testitem "AdDistributionDensity" setup=[Setup] begin
    D = @inferred FNS.Densities.Density(Normal(0.0, 0.5))
    D = @inferred FNS.Densities.Density{true}(Normal(0.0, 0.5))
    x = zeros(LogDensityProblems.dimension(D)) # ‚Ñì is your log density
    @inferred LogDensityProblems.logdensity(D)(x) # check inference, also see @code_warntype
    ds = FNS.Densities.distribution(D)
    g = gradlogpdf(ds, -0.1)
    @test g == FNS.Densities.gradlogdensity(D, -0.1)
    if isinteractive()
        @benchmark gradlogpdf($ds, -0.1)
        @benchmark FNS.Densities.gradlogdensity($D, -0.1)
        @benchmark pdf($ds, $x) # check performance and allocations
        @benchmark ($D)($x) # check performance and allocations
        @benchmark LogDensityProblems.logdensity($D, $x) # check performance and allocations
    end
    @test only(LogDensityProblems.logdensity(D, [0.1])) ==
          LogDensityProblems.logdensity(D, 0.1)
    @test_nowarn Distributions.gradlogpdf(D)(0.1)
    @inferred gradlogdensity(D, [0.1])
    @test gradlogdensity(D, [0.1]) == [-0.4]
    @test only.(LogDensityProblems.logdensity_and_gradient(D, [0.1])) ==
          LogDensityProblems.logdensity_and_gradient(D, 0.1)
end

@testitem "Overdamped Langevin Sampler" setup=[Setup] begin
    u0 = [0.0]
    tspan = (0.0, 100.0)
    S = FNS.OLE(; u0, tspan, Œ∑ = 1)

    sol = @test_nowarn solve(S, EM(); dt = 0.001, saveat = 0.01)
    x = first.(sol.u)
    plot(x)
    density(x)

    # * Try setting parameters
    s = S(Œ∑ = 0.1)
    @test s.p[1][:Œ∑] == 0.1

    D = FNS.Density(Normal(0, 10.0))
    s = S(; Œ∑ = 10.0, tspan = 1000.0, ùúã = D)
    @test s.p[1][:Œ∑] == 10.0
    @test s.p[2] == D
    @test s.tspan == 1000.0
end

@testitem "Langevin Sampler" setup=[Setup] begin
    u0 = [0.0 0.0]
    tspan = (0.0, 100.0)
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1.0, Œ≥ = 10.0)

    D = FNS.Density(Normal(0, 1))
    a = @benchmark Density($S) # Can this be made faster?
    @test a.allocs == a.memory == 0
    @test distribution(Density(S)) == D.distribution
    @test FNS.Densities.doautodiff(Density(S)) == false
    sol = @test_nowarn solve(S; dt = 0.0001, saveat = 0.01)
    x = first.(sol.u)
    plot(x)
    density(x)

    # * Try setting parameters

end

@testitem "Box boundaries" setup=[Setup] begin
    box = ReflectingBox(-5 .. 5)
    # box = FNS.NoBoundary()
    u0 = [0.0 1.0]
    tspan = (0.0, 1000.0) # Must be a matrix; col 1 is position, col2 is momentum
    # ùúã = FNS.Density(Normal(0, 0.25))
    ùúã = FNS.Density(Uniform(-5, 5)) # No potential here is pathalogical; no transient to momentum equilibrium
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1.0, Œ≥ = 0.1, boundaries = box(), ùúã)
    sol = @test_nowarn solve(S; dt = 0.001, saveat = 0.1)
    x = first.(sol.u)
    y = last.(sol.u)
    @test minimum(x) ‚â• -5 - 2e-2
    @test maximum(x) ‚â§ 5 + 2e-2
    lines(sol.t, x)
    lines(sol.t, y) # Momentum is constant?
    density(x) # The boundaries interfere with the density if they are too close
    # @test x == trajectory(S)

    box = FNS.ReflectingBox(-1 .. 1)
    u0 = [0.0 0.0]
    tspan = (0.0, 100.0)
    ùúã = FNS.Density(Normal(0, 1))
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 0.5, Œ≥ = 0.1, boundaries = box(), ùúã)
    sol = @test_nowarn solve(S; dt = 0.001, saveat = 0.01)
    x = first.(sol.u)
    y = last.(sol.u)
    minimum(x)
    lines(sol.t, x; linewidth = 3)
    lines(sol.t, y, linewidth = 3)
    density(x)
    @test minimum(x) ‚â• -1 - 0.05
    @test maximum(x) ‚â§ 1 + 0.05

    box = FNS.PeriodicBox(-1 .. 1)
    u0 = [0.0 1.0]
    tspan = (0.0, 10.0)
    ùúã = FNS.Density(Normal(0, 1))
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1, Œ≥ = 0.1, boundaries = box(), ùúã)
    sol = @test_nowarn solve(S; dt = 0.001, saveat = 0.01)
    x = first.(sol.u)
    y = last.(sol.u)
    minimum(x)
    lines(sol.t, x; linewidth = 3)
    lines(sol.t, y, linewidth = 3)
    density(x)
    @test minimum(x) ‚â• -1 - 0.02
    @test maximum(x) ‚â§ 1 + 0.02

    box = NoBoundary()
    u0 = [0.0f0 1.0f0]
    tspan = (0.0f0, 10000.0f0)
    ùúã = FNS.Density(Laplace(0.0f0, 1.0f0), true)
    S = FNS.LangevinSampler(; u0, tspan, Œ≤ = 1.0f0, Œ≥ = 1.0f0, boundaries = box(), ùúã)
    # @benchmark solve(S; dt = 0.001, saveat = 0.01)
    sol = @test_nowarn solve(S; dt = 0.001f0, saveat = 0.1f0)
    x = first.(sol.u)
    density(x)
    gg = fit(Laplace, x)
    @test gg.Œº‚âà0.0f0 atol=1e-3
    @test gg.Œ∏‚âà1.0f0 atol=1e-1
end

# @testitem "Oscillations under flat potential?" setup=[Setup] begin
if false # !! Add callbacks for discontinuities
    u0 = [0.0 0.0]
    tspan = (0.0, 100.0)

    # * Quadratic potential (gaussian pdf)
    ùúã = Normal(0.0, 1.0) |> Density
    S = FNS.LangevinSampler(; u0, tspan, ùúã, Œ≤ = 1.0, Œ≥ = 0.1)
    sol = solve(S; dt = 0.0001, saveat = 0.01)
    x = Timeseries(sol.t, first.(sol.u))
    plot(x) # Oscillating? Yes.
    hill(collect(x))

    # * Flat potential (uniform pdf... kind of. Discontinuity sucks. Add callback...boundary conditions...to handle this)
    ùúã = Uniform(-0.5, 0.5) |> Density
    S = FNS.LangevinSampler(; u0, tspan, ùúã, Œ≤ = 1.0, Œ≥ = 0.1, callbacks = ...)
    @test density(Density(S)) == density(ùúã)
    sol = solve(S; dt = 0.0001, saveat = 0.01)
    x = Timeseries(sol.t, first.(sol.u))
    plot(x) # Oscillating? No; divergent. Can't really handle delta gradient
    hill(collect(x))
end

# @testitem "LevyNoise" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    import FractionalNeuralSampling.NoiseProcesses.LevyNoise
    DIST = LevyNoise{false}(2.0, 0.0, 1 / sqrt(2), 0.0)
    Random.seed!(42)
    rng = Random.default_rng()
    a = DIST(rng)
    Random.seed!(42)
    b = DIST(rng)
    Random.seed!(42)
    c = rand(rng, FNS.NoiseProcesses.dist(DIST))
    @test a == b == c

    @test Base.return_types(DIST, (AbstractRNG,)) == [Float64]
    @test Base.return_types(DIST, (AbstractRNG, Matrix)) == [Matrix{Float64}]
    @test Base.return_types(DIST, (AbstractRNG, Type{Float64})) == [Float64]
    @test DIST(rng, randn(10, 10)) isa Matrix
    @test DIST(rng, Float64) isa Float64
    @test_throws MethodError DIST(rng, Float32)  # Method error on type mismatch
    x = StaticArraysCore.SMatrix{3, 3}(zeros(3, 3))
    Random.seed!(42)
    y = DIST(x, nothing, 0.01, nothing, nothing, nothing, rng)
    @test typeof(y) == typeof(x)

    DIST = LevyNoise{true}(2.0, 0.0, 1 / sqrt(2), 0.0)
    x = zeros(10)
    DIST(rng, x)
    @test all(x .!= 0)
    @test length(unique(x)) == length(x)
    x = zeros(10, 10)
    Random.seed!(42)
    DIST(rng, x)
    @test all(x .!= 0)
    @test length(unique(x)) == length(x)

    z = zeros(3, 3)
    Random.seed!(42)
    DIST(z, nothing, 0.01, nothing, nothing, nothing, rng)
    @test all(z .* 0.01 .^ (1 / DIST.Œ±) .== y)
end

@testitem "Test that adaptive stepping is disabled for FractionalNeuralSamplers" setup=[Setup] begin end

@testitem "FractionalNeuralSampling.jl" setup=[Setup] begin
    include("fractional_sampling.jl")
end

# @testitem "LevyProcess" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    rng = Random.default_rng()
    Random.seed!(rng, 42)
    W = LevyProcess(2.0; rng)
    dt = 0.1
    W.dt = dt
    u = nothing
    p = nothing # for state-dependent distributions
    calculate_step!(W, dt, u, p)
    for i in 1:10
        accept_step!(W, dt, u, p)
    end
    Random.seed!(rng, 42)
    prob = NoiseProblem(LevyProcess(2.0; rng, reseed = false), (0.0, 1.0))
    sol = solve(prob; dt = 0.1)

    function f3(u, p, t, W)
        2u * sin(W)
    end
    Random.seed!(rng, 42)
    u0 = 1.00
    tspan = (0.0, 5.0)
    prob = RODEProblem(f3, u0, tspan; noise = LevyProcess(2.0))
    @time sol = solve(prob, RandomEM(), dt = 1 / 100)
    plot(sol)

    function f4(du, u, p, t, W)
        du[1] = 2u[1] * sin(W[1] - W[2])
        du[2] = -2u[2] * cos(W[1] + W[2])
    end
    u0 = [1.00; 1.00]
    tspan = (0.0, 5.0)
    prob = RODEProblem(f4, u0, tspan; noise = LevyProcess(2.0))
    @test_throws "BoundsError" solve(prob, RandomEM(), dt = 1 / 100)
    @test_throws "DomainError" NoiseProblem(LevyProcess(-1.0), (0.0, 1.0))

    function f3!(u0, u, p, t, W)
        u0[1] = 2u[1] * sin(W[1])
    end
    u0 = [1.00]
    tspan = (0.0, 5.0)
    L = LevyProcess!(2.0)
    prob = RODEProblem{true}(f3!, u0, tspan; noise = L)
    @time solve(prob, RandomEM(); dt = 1 / 100)
end

# @testitem "Brownian Noise" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    prob = NoiseProblem(LevyProcess(2.0), (0.0, 1.0))
    dt = 0.00001
    ensemble = EnsembleProblem(prob)
    sol = solve(prob, RandomEM(); dt)

    lines(sol.t, sol.u; linewidth = 2)
    @test std(diff(sol.u))‚âàsqrt(dt) rtol=1e-2
end

# @testitem "Levy Noise" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    L = LevyProcess(1.5)
    prob = NoiseProblem(L, (0.0, 1.0))
    dt = 1e-6
    sol = solve(prob, RandomEM(); dt)

    g = fit(Stable, diff(sol.u) ./ (dt^(1 / 1.5)))
    @test L.dist.Œ±‚âàg.Œ± atol=1e-2
end

# @testitem "Ensemble" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    Random.seed!(42)
    L = LevyProcess(1.5)
    dt = 1e-3
    prob = NoiseProblem(L, (0.0, 1.0))
    ensemble = EnsembleProblem(prob)
    sol = @test_nowarn solve(ensemble, RandomEM(), EnsembleSerial(); trajectories = 5, dt)
    @test_nowarn solve(ensemble, RandomEM(), EnsembleDistributed(); trajectories = 5, dt)
    @test_nowarn solve(ensemble, RandomEM(), EnsembleThreads(); trajectories = 5, dt)
    g = Figure()
    ax = Axis(g[1, 1])
    [lines!(ax, s.t, s.u) for s in sol]
    display(g)
end

# @testitem "Benchmark LevyNoise" setup=[Setup] begin
if false # ! Need to fix out-of-place noise
    import FractionalNeuralSampling.NoiseProcesses.LevyNoise
    import FractionalNeuralSampling.NoiseProcesses.LevyNoise!
    import DiffEqNoiseProcess.WHITE_NOISE_DIST as W
    import DiffEqNoiseProcess.INPLACE_WHITE_NOISE_DIST as W!
    L = LevyNoise(2.0, 0.0, 1 / sqrt(2), 0.0)
    L! = LevyNoise!(2.0, 0.0, 1 / sqrt(2), 0.0)
    rng = Random.default_rng()

    X = zeros(100, 100)
    _L = Stable(2.0, 0.0, 1 / sqrt(2), 0.0)
    a = @benchmark randn(size($X))
    c = @benchmark $L($rng, $X)
    @test a.memory‚âàc.memory atol=10
    @test a.allocs == c.allocs == 2

    a = @benchmark $W($X, 0.0, 0.01, 0.0, 0.0, 0.0, $rng)
    b = @benchmark $L($X, 0.0, 0.01, 0.0, 0.0, 0.0, $rng)
    c = @benchmark $W!($X, 0.0, 0.01, 0.0, 0.0, 0.0, $rng)
    d = @benchmark $L!($X, 0.0, 0.01, 0.0, 0.0, 0.0, $rng)
    @test c.allocs == d.allocs == 0
    @test c.memory‚âàd.memory atol=10
    @test a.allocs == b.allocs == 4
    @test a.memory‚âàb.memory atol=10

    a = @benchmark rand!($rng, Stable(2.0, 0.0, 1 / sqrt(2), 0.0), $X)
    a = @benchmark $L!($rng, $X)
    @test a.allocs == a.memory == 0

    @benchmark Stable(2.0, 0.0, 1 / sqrt(2), 0.0) # * Super cheap
end

if CUDA.functional(true)
    @testitem "GPU Benchmark" setup=[Setup] begin
        using DiffEqGPU
        function f3!(u0, u, p, t, W)
            u0[1] = 2u[1] * sin(W[1])
        end
        u0 = [1.00]
        tspan = (0.0, 5.0)
        dt = 0.01
        L = LevyProcess!(2.0)
        prob = RODEProblem{true}(f3!, u0, tspan; noise = L)
        ensemble = EnsembleProblem(prob)
        @test_nowarn @benchmark solve($ensemble, RandomEM(), EnsembleSerial();
                                      trajectories = 5,
                                      dt = $dt)
        @test_throws "MethodError" solve(ensemble, RandomEM(),
                                         EnsembleGPUArray(CUDA.CUDABackend());
                                         trajectories = 5, dt)
    end
end

# @testitem "2D potential" setup=[Setup] begin
if false
    using DifferentialEquations
    Œîx = 5
    d = MixtureModel([
                         MvNormal([-Œîx / 2, 0], [1 0; 0 1]),
                         MvNormal([Œîx / 2, 0], [1 0; 0 1])
                     ])
    D = Density(d)

    Œ±s = [2.0, 1.6, 1.2]
    f = Figure(size = (900, 300))
    gs = subdivide(f, 1, 3)
    map(Œ±s, gs) do Œ±, g
        L = FractionalNeuralSampler(;
                                    u0 = [-Œîx / 2 0 0 0],
                                    tspan = 500.0,
                                    Œ± = Œ±,
                                    Œ≤ = 0.2,
                                    Œ≥ = 0.02,
                                    ùúã = D,
                                    seed = 42)
        sol = solve(L, EM(); dt = 0.001)
        x, y = eachrow(sol[1:2, :])

        xmax = maximum(abs.(extrema(vcat(x, y)))) * 1.5
        xs = range(-xmax, xmax, length = 100)

        ax = Axis(g[1, 1], title = "Œ± = $Œ±", aspect = DataAspect())
        # heatmap!(ax, xs, xs, potential(D).(collect.(Iterators.product(xs, xs))), colormap=seethrough(:turbo))
        heatmap!(ax, xs, xs, D.(collect.(Iterators.product(xs, xs))),
                 colormap = seethrough(:turbo))
        lines!(ax, x[1:10:end], y[1:10:end], color = (:black, 0.5), linewidth = 1)
        hidedecorations!(ax)
    end
    f
end

# @testitem "MSD check" setup=[Setup] begin
begin
    u0 = [0.0, 0.0]
    tspan = (0.0, 10000.0)
    dt = 0.1
    D = FNS.Density(Normal(0.0, 1.0))
    S = FNS.Samplers.FractionalHMC(; u0, tspan, Œ± = 1.9, Œ≤ = 0.01, Œ≥ = 1.0, ùúã = D)
    sol = solve(S, EM(); dt)

    x = sol[1, :]
    x = TimeseriesTools.Timeseries(sol.t, x)
    x = rectify(x, dims = ùë°, tol = 1)

    msd = msdist(x)

    begin
        f = Figure(size = (1000, 300))
        ax = Axis(f[1, 1])
        lines!(ax, x[1:20:20000])

        lu = (-4 * std(x), +4 * std(x))
        axx = Axis(f[1, 2], limits = (lu, nothing))
        xs = range(lu..., length = 1000)
        hist!(axx, x; bins = 100, normalization = :pdf)
        lines!(axx, xs, D.(xs); color = :red, linewidth = 2)

        axxx = Axis(f[1, 3], xscale = log10, yscale = log10)
        lines!(axxx, msd[ùë° = dt .. 1000], label = nothing)

        # * Fit a tail index to msd
        y = msd[ùë° = dt .. 1]
        ts = logrange(extrema(times(y))..., length = 1000)
        y = y[ùë° = Near(ts)]
        taus = times(y)
        Œ±, Œ≤ = [log10.(taus) ones(length(y))] \ log10.(y)

        # * Plot line of fitted tail
        lines!(axxx, taus, 10 .^ (Œ± * log10.(taus) .+ Œ≤); color = :red, linewidth = 2,
               label = "Fit: Œ± = $Œ±")
        axislegend(axxx, position = :lt)
        display(f)
    end
end

begin
    x = rand(Stable(2.0, 0.0), 10000) |> cumsum
    x = Timeseries(range(dt, length = length(x), step = dt), x)
    lines(x)
    msd = mad(x)
    f = Figure()
    ax = Axis(f[1, 1], xscale = log10, yscale = log10)
    lines!(ax, msd[ùë° = dt .. 100], label = nothing)

    # * Fit a tail index to msd
    y = msd[ùë° = dt .. 1]
    ts = logrange(extrema(times(y))..., length = 1000)
    y = y[ùë° = Near(ts)]
    taus = times(y)
    Œ±, Œ≤ = [log10.(taus) ones(length(y))] \ log10.(y)

    # * Plot line of fitted tail
    lines!(ax, taus, 10 .^ (Œ± * log10.(taus) .+ Œ≤); color = :red, linewidth = 2,
           label = "Fit: Œ± = $Œ±")
    axislegend(ax, position = :lt)
    display(f)
end

begin
    using TimeseriesTools
    using CairoMakie
    using StableDistributions
    using Statistics
    function mad(x::AbstractVector{T}) where {T <: Real}
        n = length(x)
        lags = 1:(n - 1)
        mads = zeros(T, n - 1)
        Threads.@threads for lag in lags
            displacements = abs.(x[(1 + lag):n] .- x[1:(n - lag)])
            mads[lag] = mean(displacements)
        end
        return mads
    end
    function mad(x::UnivariateRegular)
        mads = mad(parent(x))
        lags = range(step(x), length = length(mads), step = step(x))
        return Timeseries(lags, mads)
    end
    function mssd(x::AbstractVector{T}) where {T <: Real}
        n = length(x)
        lags = 1:(n - 1)
        msds = zeros(T, n - 1)
        Threads.@threads for lag in lags
            displacements = (x[(1 + lag):n] .- x[1:(n - lag)]) .^ 2
            msds[lag] = mean(displacements)
        end
        return msds
    end
    function mssd(x::UnivariateRegular)
        mssds = mssd(parent(x))
        lags = range(step(x), length = length(mssds), step = step(x))
        return Timeseries(lags, mssds)
    end
end

begin
    x = rand(Stable(1.3, 0.0), 10000) |> cumsum
    x = Timeseries(range(dt, length = length(x), step = dt), x)
    lines(x)
    msd = mad(x)
    f = Figure()
    ax = Axis(f[1, 1], xscale = log10, yscale = log10)
    lines!(ax, msd[ùë° = dt .. 100], label = nothing)

    # * Fit a tail index to msd
    y = msd[ùë° = dt .. 1]
    ts = logrange(extrema(times(y))..., length = 1000)
    y = y[ùë° = Near(ts)]
    taus = times(y)
    Œ±, Œ≤ = [log10.(taus) ones(length(y))] \ log10.(y)

    # * Plot line of fitted tail
    lines!(ax, taus, 10 .^ (Œ± * log10.(taus) .+ Œ≤); color = :red, linewidth = 2,
           label = "Fit: Œ± = $Œ±")
    axislegend(ax, position = :lt)
    display(f)
end

begin
    global estimator = mad
    repeats = 100
    dt = 0.01 # Dummy timestep
    Œ±s = 1.1:0.05:2.0

    ms = progressmap(Œ±s) do Œ±
        map(1:repeats) do _
            x = rand(Stable(Œ±, 0.0), 10000) |> cumsum
            x = Timeseries(range(dt, length = length(x), step = dt), x)
            m = estimator(x)

            # * Fit a tail index to msd
            y = m[ùë° = dt .. 1]
            ts = logrange(extrema(times(y))..., length = 1000)
            y = y[ùë° = Near(ts)]
            taus = times(y)
            Œ≤, _ = [log10.(taus) ones(length(y))] \ log10.(y)
            return Œ≤
        end
    end
    œÉs = std.(ms) ./ 2
    ms = mean.(ms)

    f = Figure()
    ax = Axis(f[1, 1], xlabel = "Œ±", ylabel = "Œ≤", title = "$estimator")
    band!(ax, Œ±s, ms .- œÉs, ms .+ œÉs; color = (:black, 0.3))
    lines!(ax, Œ±s, ms, color = :black)
    if estimator === mad
        # * Plot a line of beta = 1/alpha
        lines!(ax, Œ±s, 1 ./ Œ±s, color = :red, linestyle = :dash, label = "Œ≤ = 1/Œ±")
    end
    display(f)
end

@testitem "CaputoEM" begin
    include("./Solvers/CaputoEM.jl")
end
